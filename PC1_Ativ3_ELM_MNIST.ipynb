{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python388jvsc74a57bd02fe6452616a5c56ac366cd71f0310863e24cc0ef48f8e1d866f66e4662847f82",
      "display_name": "Python 3.8.8 64-bit ('test')"
    },
    "metadata": {
      "interpreter": {
        "hash": "2fe6452616a5c56ac366cd71f0310863e24cc0ef48f8e1d866f66e4662847f82"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Notebook PC1_Ativ3**\n",
        "## Regularized ELM Classifier for the MNIST database.\n",
        "**Professor:** Fernando J. Von Zuben <br>\n",
        "**Aluno(a):**\n"
      ],
      "metadata": {
        "id": "xMsNyA3WPrER"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOfS3mVy1su0"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(Xp, y), (Xtp, yt) = mnist.load_data()\n",
        "Xa = Xp.reshape(Xp.shape[0], 784)\n",
        "Xta = Xtp.reshape(Xtp.shape[0], 784)\n",
        "\n",
        "X = Xa / 255.0\n",
        "Xt = Xta / 255.0\n",
        "\n",
        "print(\"Shape of X: \".ljust(10),  X.shape)\n",
        "print(\"Shape of y: \".ljust(10),  y.shape)\n",
        "print(\"Shape of Xt: \".ljust(10),  Xt.shape)\n",
        "print(\"Shape of yt: \".ljust(10),  yt.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "yy = to_categorical(y)\n",
        "yyt = to_categorical(yt)\n",
        "sum_along_columns1 = np.sum(yy, axis = 0)\n",
        "print(sum_along_columns1)\n",
        "sum_along_columns2 = np.sum(yyt, axis = 0)\n",
        "print(sum_along_columns2)"
      ],
      "metadata": {
        "id": "1C0i2D-gAThk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2_oRRd-1su3"
      },
      "outputs": [],
      "source": [
        "partition = 0.7\n",
        "\n",
        "# Mixing the dataset before partitioning\n",
        "index = np.arange(0,X.shape[0])\n",
        "np.random.shuffle(index)\n",
        "\n",
        "training_set_size = int(X.shape[0] * partition)\n",
        "\n",
        "index_training = index[:training_set_size]\n",
        "index_validation = index[training_set_size:]\n",
        "\n",
        "\n",
        "X_training = X[index_training]\n",
        "y_training = yy[index_training]\n",
        "\n",
        "X_validation = X[index_validation]\n",
        "y_validation = yy[index_validation]\n",
        "\n",
        "# ELM X\n",
        "# We will concatenate a column of 1's to account for the V0 of each hidden neuron later on\n",
        "X_training_elm = np.concatenate((np.transpose(np.array([np.ones(X_training.shape[0])])), X_training), axis=1)\n",
        "X_validation_elm = np.concatenate((np.transpose(np.array([np.ones(X_validation.shape[0])])), X_validation), axis=1)\n",
        "\n",
        "print(\"X_training:\".ljust(20), X_training.shape)\n",
        "print(\"y_training:\".ljust(20), y_training.shape)\n",
        "\n",
        "print(\"X_validation:\".ljust(20), X_validation.shape)\n",
        "print(\"y_validation:\".ljust(20), y_validation.shape)\n",
        "\n",
        "print(\"X_training_elm:\".ljust(20), X_training_elm.shape)\n",
        "print(\"X_validation_elm:\".ljust(20), X_validation_elm.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_class_from_sample(sample: np.ndarray):\n",
        "    class_ = 1\n",
        "    for i in sample:\n",
        "        if i == 0:\n",
        "            class_ += 1\n",
        "        else:\n",
        "            break\n",
        "    return class_ "
      ],
      "metadata": {
        "id": "NvFzhLJD4ZSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT15bocK1su4"
      },
      "outputs": [],
      "source": [
        "def leaky_relu(table):\n",
        "    return np.where(table > 0, table,  0.02 * table)\n",
        "\n",
        "def relu(table):\n",
        "    return np.where(table > 0, table,  0)\n",
        "\n",
        "def get_h(X, V, activation_function): \n",
        "    V_ = np.matmul(X, V)\n",
        "    return activation_function(V_)\n",
        "\n",
        "def get_w(X, y, c): \n",
        "    w_1 = (np.matmul(np.transpose(X), X) + c * np.eye(X.shape[1]))\n",
        "    w_2 = np.matmul(np.transpose(X), y)\n",
        "    w,resid,rank,s = np.linalg.lstsq(w_1, w_2, rcond=None)\n",
        "    return w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "IiMpyDst1su4"
      },
      "outputs": [],
      "source": [
        "def get_rates_from_c_values(X, y, c_values, Ws = []):\n",
        "    hit_rates = []\n",
        "    \n",
        "    if len(Ws) != 0:\n",
        "        are_Ws_given = True\n",
        "    else:\n",
        "        are_Ws_given = False\n",
        "        Ws = []\n",
        "\n",
        "    for index, c in enumerate(c_values):\n",
        "        print(c)\n",
        "        \n",
        "        if are_Ws_given:\n",
        "            W = Ws[index]\n",
        "        else:\n",
        "            W = get_w(X, y, c)\n",
        "            Ws.append(W)\n",
        "\n",
        "        y_estimate = np.matmul(X,W)\n",
        "        hits = 0\n",
        "        for index, estimate in enumerate(y_estimate):\n",
        "            max_index = np.where(estimate == np.amax(estimate))[0][0]\n",
        "            estimated_class = max_index + 1\n",
        "            if estimated_class == get_class_from_sample(y[index]):\n",
        "                hits += 1\n",
        "        hit_rates.append(hits/y_estimate.shape[0])\n",
        "    return hit_rates, Ws"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard deviation of the random weight generation at the hidden layer\n",
        "sigma = 0.2\n",
        "# Number of inputs for the MNIST dataset: 785\n",
        "# Number of inputs for the CIFAR10 dataset: 3073\n",
        "# Take 1000 hidden neurons for the MNIST dataset and 2000 for the CIFAR10 dataset.\n",
        "V  = sigma * np.random.randn(785, 1000)"
      ],
      "metadata": {
        "id": "tTrKQKFcEfCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_dbJ39H1su5"
      },
      "outputs": [],
      "source": [
        "c_values = [pow(2,d) for d in range(-16, 14, 2)]\n",
        "\n",
        "hit_rates_training, Ws = get_rates_from_c_values(get_h(X_training_elm, V, relu), y_training, c_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3y_MVJjD1su6"
      },
      "outputs": [],
      "source": [
        "hit_rates_validation, _ = get_rates_from_c_values(get_h(X_validation_elm, V, relu), y_validation, c_values, Ws)"
      ]
    },
    {
      "source": [
        "fig, axs = plt.subplots(1, 2)\n",
        "fig.set_figwidth(10)\n",
        "fig.suptitle(\"Crude search for the regularization coefficient\")\n",
        "axs[0].semilogx(c_values, hit_rates_training, 'o-')\n",
        "axs[0].set_title(\"Performance on Training set\")\n",
        "axs[0].set_ylabel(\"Hit Rate\")\n",
        "axs[0].set_xlabel(\"Regularization Coefficient 'c'\")\n",
        "axs[0].grid()\n",
        "\n",
        "axs[1].semilogx(c_values, hit_rates_validation, 'o-')\n",
        "axs[1].set_title(\"Performance on Validation set\")\n",
        "axs[1].set_ylabel(\"Hit Rate\")\n",
        "axs[1].set_xlabel(\"Regularization Coefficient 'c'\")\n",
        "axs[1].grid()\n",
        "\n",
        "best_c_index = np.where(hit_rates_validation == np.amax(hit_rates_validation))[0][0]\n",
        "\n",
        "best_c = c_values[best_c_index]\n",
        "\n",
        "print(\"Best c value: {} \\nPerformance of this value: {}\".format(best_c, hit_rates_validation[best_c_index]))\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "puchmu481su6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGfLZjNO1su6"
      },
      "outputs": [],
      "source": [
        "step =  0.1 * (4 * best_c - best_c / 4);\n",
        "\n",
        "fine_c_values = np.arange((best_c/4), (4*best_c) + step / 10, step)\n",
        "\n",
        "hit_rates_training_fine, Ws_fine = get_rates_from_c_values(get_h(X_training_elm, V, relu), y_training, fine_c_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zeu_IRjh1su7"
      },
      "outputs": [],
      "source": [
        "hit_rates_validation_fine, _ = get_rates_from_c_values(get_h(X_validation_elm, V, relu), y_validation, fine_c_values, Ws_fine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cmgx2-Ea1su7"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 2)\n",
        "fig.set_figwidth(10)\n",
        "fig.suptitle(\"Fine search for the regularization coefficient\")\n",
        "axs[0].plot(fine_c_values, hit_rates_training_fine, 'o-')\n",
        "axs[0].set_title(\"Performance on Training set\")\n",
        "axs[0].set_ylabel(\"Hit Rate\")\n",
        "axs[0].set_xlabel(\"Regularization Coefficient 'c'\")\n",
        "axs[0].grid()\n",
        "\n",
        "axs[1].plot(fine_c_values, hit_rates_validation_fine, 'o-')\n",
        "axs[1].set_title(\"Performance on Validation set\")\n",
        "axs[1].set_ylabel(\"Hit Rate\")\n",
        "axs[1].set_xlabel(\"Regularization Coefficient 'c'\")\n",
        "axs[1].grid()\n",
        "\n",
        "best_c_index = np.where(hit_rates_validation_fine == np.amax(hit_rates_validation_fine))[0][0]\n",
        "\n",
        "best_c = fine_c_values[best_c_index]\n",
        "\n",
        "print(\"Best c value: {} \\nPerformance of this value: {}\".format(best_c, hit_rates_validation_fine[best_c_index]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkzD4sCG1su8"
      },
      "outputs": [],
      "source": [
        "X_elm = np.concatenate((np.transpose(np.array([np.ones(X.shape[0])])), X), axis=1)\n",
        "W_elm = get_w(get_h(X_elm, V, relu), yy, best_c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "3GyVL_ib1su8"
      },
      "outputs": [],
      "source": [
        "Xt_elm = np.concatenate((np.transpose(np.array([np.ones(Xt.shape[0])])), Xt), axis=1)\n",
        "yt_estimate = np.matmul(get_h(Xt_elm, V, relu),W_elm)\n",
        "hits = 0\n",
        "confusion_matrix = np.zeros([10, 10], dtype=int)\n",
        "for index, estimate in enumerate(yt_estimate):\n",
        "    max_index = np.where(estimate == np.amax(estimate))[0][0]\n",
        "    estimated_class = max_index + 1\n",
        "    if estimated_class == get_class_from_sample(yyt[index,:]):\n",
        "        hits += 1\n",
        "        confusion_matrix[estimated_class-1][estimated_class-1] += 1\n",
        "    else:\n",
        "        confusion_matrix[estimated_class-1][get_class_from_sample(yyt[index,:])-1] += 1\n",
        "\n",
        "print(\"Performance on test set: {}\".format(hits/yt_estimate.shape[0]))\n",
        "print(confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Devoted solely to the MNIST dataset\n",
        "def find_pos(item, lst): \n",
        "    pos = [i for (z, i) in zip(lst, range(len(lst))) if item == z] \n",
        "    return pos\n",
        "\n",
        "def display_sample(sample: np.ndarray):\n",
        "\n",
        "    sample = np.array(sample, dtype='float')\n",
        "    pixels = np.zeros((28,28))\n",
        "\n",
        "    for i in range(28):\n",
        "        for j in range(28):\n",
        "            pixels[i,j] = sample[i * 28 + j]\n",
        "\n",
        "    pixels = pixels.reshape((28, 28))\n",
        "    plt.imshow(pixels, cmap='viridis')\n",
        "    plt.show()\n",
        "\n",
        "yt_estimate = np.matmul(get_h(Xt_elm, V, relu),W_elm)\n",
        "for i in range(200):\n",
        "    if yyt[i][np.argmax(yt_estimate[i,:])] != 1:\n",
        "      print(f\"Image no. {i}\")\n",
        "      display_sample(Xt[i])\n",
        "      print(f\"Real: {np.argmax(yyt[i,:])}\")\n",
        "      print(f\"Predicted: {np.argmax(yt_estimate[i,:])}\")\n",
        "      value = yt_estimate[i,np.argmax(yyt[i,:])]\n",
        "      v_sorted = np.sort(yt_estimate[i,:])\n",
        "      value_index = find_pos(value,v_sorted)\n",
        "      print(f\"Rank of the correct answer: {10 - value_index[0]}\")\n",
        "      print(f\"Probabilities: {yt_estimate[i,:]} \\n\")"
      ],
      "metadata": {
        "id": "7MQGRUNGKv-C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}